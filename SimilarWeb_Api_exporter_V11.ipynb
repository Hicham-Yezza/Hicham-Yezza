{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOPf5F58ifH+JCgdgaENi6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# SimilarWeb API Data Extractor - Prototype V11\n",
        "# ---------------------------------------------\n",
        "# Author: Hicham Yezza\n",
        "# Date: March 2025\n",
        "# Descritpion:\n",
        "# A script for extracting traffic data from the SimilarWeb API\n",
        "\n",
        "# WARNINNG: Do not use for BBCM editorial purposes unless in conjunction with the Data Hub team - please do not share the API key with anyone\n",
        "\n",
        "# Usage:\n",
        "# 1- Enter your SimilarWeb API key when asked\n",
        "# 2- Either upload a file with website URLs (CSV or xlsx) or type in a comma-separate list of websites (eg bbc.co.uk, Sana.sy )\n",
        "# 3- Pick the sheet and column you need\n",
        "# 4- Define start and end date for the data (please pay attention to the formatting guidance)\n",
        "# 5- Choose the granularity need (i.e. how detailed the data should be -- daily, weekly, monthy)\n",
        "# 6- Final data is saved as a CSV into your default folder\n",
        "# 7- Open csv in Excel as usual\n",
        "# use pivot tables to organise/navigate"
      ],
      "metadata": {
        "id": "gTiT6s1H1Au_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Key updates in V11:\n",
        "- Added num selection for sheet/column\n",
        "- Added granularity selection (daily, weekly, monthy)\n",
        "- Improved error logging to address recurring API issues (401, 404, 429, 500)\n",
        "- Fixed domain extraction (was causing a few odd issues before)\n",
        "- Better rate limit handling - using adaptive backoff (for now seems to avoid previous issues)\n",
        "- Using urlparse for handling URL format issues (still not 100% robust)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fh_qCAhTwZuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies (if not already installed)\n",
        "!pip install pandas requests tqdm"
      ],
      "metadata": {
        "id": "STVrYyy8wZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from urllib.parse import urlparse"
      ],
      "metadata": {
        "id": "wOif_758wZ0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging (need to decide how to store/surface long term)\n",
        "logging.basicConfig(filename=\"similarweb_log.txt\", level=logging.INFO,\n",
        "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")"
      ],
      "metadata": {
        "id": "uT9OxSZ-wZ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API key from user\n",
        "API_KEY = input(\"Enter your SimilarWeb API key: \").strip()\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"API key is required.\")"
      ],
      "metadata": {
        "id": "AKeOlOnYwZ40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload file or enter URLs manually\n",
        "choice = input(\"Do you want to upload a file with URLs? (yes/no): \").strip().lower()\n",
        "\n",
        "if choice == 'yes':\n",
        "    # Upload file containing website list\n",
        "    print(\"Please upload your file.\")\n",
        "    uploaded = files.upload()\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # Load websites from uploaded file\n",
        "    if file_path.endswith('.xlsx'):\n",
        "        xls = pd.ExcelFile(file_path)\n",
        "        print(\"\\nAvailable sheets:\")\n",
        "        for idx, sheet in enumerate(xls.sheet_names):\n",
        "            print(f\"{idx+1}: {sheet}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                sheet_idx = int(input(\"Pick sheet number: \").strip()) - 1\n",
        "                if 0 <= sheet_idx < len(xls.sheet_names):\n",
        "                    sheet_name = xls.sheet_names[sheet_idx]\n",
        "                    break\n",
        "                print(\"Invalid choice. Try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Enter a number.\")\n",
        "\n",
        "    df_websites = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "\n",
        "    # Pick column containing URLs\n",
        "    print(\"\\nColumns available:\")\n",
        "    for idx, col in enumerate(df_websites.columns):\n",
        "        print(f\"{idx+1}: {col}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            column_idx = int(input(\"Enter column number for URLs: \").strip()) - 1\n",
        "            if 0 <= column_idx < len(df_websites.columns):\n",
        "                column_name = df_websites.columns[column_idx]\n",
        "                break\n",
        "            print(\"Invalid selection, try again.\")\n",
        "        except ValueError:\n",
        "            print(\"Enter a number.\")\n",
        "\n",
        "    websites = df_websites[column_name].dropna().astype(str).tolist()\n",
        "\n",
        "else:\n",
        "    # Direct input by user as comma-separated values\n",
        "    websites_input = input(\"Enter website URLs separated by commas: \").strip()\n",
        "    websites = [w.strip() for w in websites_input.split(\",\") if w.strip()]"
      ],
      "metadata": {
        "id": "blPmJKXGwsvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract domain names (handles missing http/https, removes www.)\n",
        "def extract_domain(url):\n",
        "    \"\"\"Ensure URL has a scheme (http/https), then extract domain.\"\"\"\n",
        "    try:\n",
        "        url = url.strip()  # Remove leading/trailing spaces\n",
        "\n",
        "        if not url:\n",
        "            return None  # Ignore empty values\n",
        "\n",
        "        if not url.startswith((\"http://\", \"https://\")):\n",
        "            url = \"http://\" + url  # Default to http if missing\n",
        "\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc.strip()\n",
        "\n",
        "        if not domain:\n",
        "            return None  # Ensure non-empty domain\n",
        "\n",
        "        return domain.replace(\"www.\", \"\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process user-provided URLs\n",
        "websites = list(set(filter(None, [extract_domain(w) for w in websites])))\n",
        "\n",
        "if not websites:\n",
        "    print(\"No valid domains provided. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Filtered domains:\", websites)"
      ],
      "metadata": {
        "id": "Atqv4rRh-DNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Date and granularity selection\n",
        "def validate_date(date_str, date_format):\n",
        "    \"\"\"Checks if date is in correct format\"\"\"\n",
        "    try:\n",
        "        datetime.strptime(date_str, date_format)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "valid_granularities = ['daily', 'weekly', 'monthly']\n",
        "while True:\n",
        "    GRANULARITY = input(\"Select granularity (daily/weekly/monthly): \").strip().lower()\n",
        "    if GRANULARITY in valid_granularities:\n",
        "        break\n",
        "    print(\"Invalid choice, please enter daily, weekly, or monthly.\")\n",
        "\n",
        "DATE_FORMAT = \"%Y-%m-%d\" if GRANULARITY in ['daily', 'weekly'] else \"%Y-%m\"\n",
        "\n",
        "while True:\n",
        "    START_DATE = input(f\"Start date ({DATE_FORMAT}): \").strip()\n",
        "    END_DATE = input(f\"End date ({DATE_FORMAT}): \").strip()\n",
        "\n",
        "    if validate_date(START_DATE, DATE_FORMAT) and validate_date(END_DATE, DATE_FORMAT):\n",
        "        break\n",
        "    print(f\"Invalid date format. Use {DATE_FORMAT}.\")"
      ],
      "metadata": {
        "id": "aiAKM74_wy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API request function ### with rate limit handling but need to revisit #AP\n",
        "def fetch_api_data(website, api_key, retries=5, base_wait=5):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (DataExtractor/1.0)',\n",
        "        'Accept': 'application/json',\n",
        "    }\n",
        "\n",
        "    url = (\n",
        "        f\"https://api.similarweb.com/v1/website/{website}/\"\n",
        "        \"total-traffic-and-engagement/visits?\"\n",
        "        f\"api_key={api_key}&start_date={START_DATE}&end_date={END_DATE}\"\n",
        "        f\"&granularity={GRANULARITY}&main_domain_only=false&format=json\"\n",
        "        \"&show_verified=false&mtd=false&engaged_only=false\"\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            logging.info(f\"{website}: success\")\n",
        "            return response.json(), 200\n",
        "\n",
        "        elif response.status_code == 404:\n",
        "            logging.warning(f\"{website}: not tracked (404)\")\n",
        "            return None, 404\n",
        "\n",
        "        elif response.status_code == 401:\n",
        "            logging.error(f\"{website}: Unauthorized (401)\")\n",
        "            return None, 401\n",
        "\n",
        "        elif response.status_code == 429:  # Too many\n",
        "            retry_after = response.headers.get(\"Retry-After\")\n",
        "            if retry_after:\n",
        "                wait_time = int(retry_after) + random.uniform(1, 3)  # Add slight randomness\n",
        "            else:\n",
        "                wait_time = base_wait * (2 ** attempt)  # Exponential backoff -- to revisit #AP\n",
        "\n",
        "            logging.warning(f\"{website}: Rate limited (429). Retrying in {wait_time:.2f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        elif response.status_code == 500:  # Server\n",
        "            wait_time = base_wait * (2 ** attempt)\n",
        "            logging.warning(f\"{website}: Server issue (500). Retrying in {wait_time:.2f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        else:\n",
        "            logging.error(f\"{website}: Unexpected error ({response.status_code}): {response.text}\")\n",
        "            return None, response.status_code\n",
        "\n",
        "    logging.error(f\"{website}: Failed after {retries} retries\")\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "mGjSvcMIw146"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data\n",
        "all_data = []\n",
        "for site in tqdm(websites, desc=\"Fetching Data\"):\n",
        "    data, status_code = fetch_api_data(site, API_KEY)\n",
        "    if data is not None:\n",
        "        all_data.append({'website': site, 'data': data})"
      ],
      "metadata": {
        "id": "fI1ke1cOw17-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "if all_data:\n",
        "    df_expanded = pd.DataFrame([\n",
        "        {'website': row['website'], 'date': visit.get('date', 'N/A'), 'visits': visit.get('visits', 0)}\n",
        "        for row in all_data if isinstance(row['data'], dict)\n",
        "        for visit in row['data'].get('visits', [])\n",
        "    ])\n",
        "\n",
        "    if not df_expanded.empty:\n",
        "        export_filename = f\"Similarweb-Export-{START_DATE}-{END_DATE}-{GRANULARITY}.csv\"\n",
        "        df_expanded.round({'visits': 0}).to_csv(export_filename, index=False) # rounding off from flating point\n",
        "        print(f\"\\nData saved as {export_filename}\")\n",
        "        files.download(export_filename)\n"
      ],
      "metadata": {
        "id": "eNDBQUBUwaAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}