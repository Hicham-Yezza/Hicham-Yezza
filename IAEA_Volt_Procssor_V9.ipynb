{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hicham-Yezza/Hicham-Yezza/blob/main/IAEA_Volt_Procssor_V9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcoLoU9cRsG6"
      },
      "outputs": [],
      "source": [
        "# IAEA Volt processing script -- Updated with Country and Language Columns\n",
        "# Hicham Yezza -- Nov 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install deep-translator\n",
        "!pip install tqdm\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# nltk stopwords and tokenizer data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "# Download 'punkt_tab' if not found\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize tqdm for progress monitoring\n",
        "tqdm.pandas()\n",
        "\n",
        "# Prompt the user to upload files\n",
        "print(\"Please upload the CSV or Excel files containing the mentions data.\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Prompt the user to upload the channel list sheet\n",
        "print(\"Please upload the Excel file containing the channel list data.\")\n",
        "channel_list_file = files.upload()\n",
        "channel_data_path = list(channel_list_file.keys())[0]\n",
        "channel_data = pd.read_excel(channel_data_path)\n",
        "\n",
        "# Create a dictionary for fast lookup of country and language based on channel source code\n",
        "channel_mapping = channel_data.set_index('Channel source code')[['Language', 'Country']].to_dict('index')\n",
        "\n",
        "# Initialize NLTK stopwords for filtering irrelevant words\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load spaCy's small English model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a strict mapping between Arabic variants and the 15 English keywords\n",
        "mapping_dict = {\n",
        "    # Nuclear fuel variations\n",
        "    \"الوقود النووي\": \"Nuclear fuel\",\n",
        "    \"بالوقود النووي\": \"Nuclear fuel\",\n",
        "    \"الوقود النووية\": \"Nuclear fuel\",\n",
        "    \"الوقود\": \"Nuclear fuel\",\n",
        "    \"للوقود\": \"Nuclear fuel\",\n",
        "    \"الوقود المستهلكة\": \"Nuclear fuel\",\n",
        "    \"الوقود والمستهلكات\": \"Nuclear fuel\",\n",
        "\n",
        "    # Nuclear reactor variations\n",
        "    \"مفاعل نووي\": \"Nuclear reactor\",\n",
        "    \"مفاعل نووية\": \"Nuclear reactor\",\n",
        "    \"المفاعلات النووية\": \"Nuclear reactor\",\n",
        "    \"للمفاعلات النووية\": \"Nuclear reactor\",\n",
        "    \"المفاعل النووي\": \"Nuclear reactor\",\n",
        "    \"المفاعل النووية\": \"Nuclear reactor\",\n",
        "    \"مفاعلات نووية\": \"Nuclear reactor\",\n",
        "    \"مفاعلها النووية\": \"Nuclear reactor\",\n",
        "    \"والمفاعلات النووية\": \"Nuclear reactor\",\n",
        "    \"بالمفاعلات النووية\": \"Nuclear reactor\",\n",
        "    \"للمفاعل النووي\": \"Nuclear reactor\",\n",
        "    \"بالمفاعل النووي\": \"Nuclear reactor\",\n",
        "    \"ومفاعلات نووية\": \"Nuclear reactor\",\n",
        "    \"مفاعلين نوويين\": \"Nuclear reactor\",\n",
        "    \"والمفاعل النووي\": \"Nuclear reactor\",\n",
        "    \"والمفاعل النووية\": \"Nuclear reactor\",\n",
        "    \"مفاعلاتها النووية\": \"Nuclear reactor\",\n",
        "    \"المفاعلات النوويه\": \"Nuclear reactor\",\n",
        "    \"مفاعلها النووي\": \"Nuclear reactor\",\n",
        "    \"مفاعل النووي\": \"Nuclear reactor\",\n",
        "    \"للمفاعل النووية\": \"Nuclear reactor\",\n",
        "    \"مفاعلات النووية\": \"Nuclear reactor\",\n",
        "    \"والمفاعلاتها النووية\": \"Nuclear reactor\",\n",
        "    \"والمفاعلات النوويه\": \"Nuclear reactor\",\n",
        "    \"المفاعل\": \"Nuclear reactor\",\n",
        "\n",
        "    # Nuclear power plant variations\n",
        "    \"محطة للطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"محطات الطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"محطة طاقة نووية\": \"Nuclear power plant\",\n",
        "    \"محطات طاقة نووية\": \"Nuclear power plant\",\n",
        "    \"محطات الطاقه النوويه\": \"Nuclear power plant\",\n",
        "    \"المحطات الطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"المحطة للطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"محطة الطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"محطه الطاقه النوويه\": \"Nuclear power plant\",\n",
        "    \"محطات للطاقة النووية\": \"Nuclear power plant\",\n",
        "    \"محطة الطاقة\": \"Nuclear power plant\",\n",
        "\n",
        "    # Uranium variations\n",
        "    \"اليورانيوم\": \"Uranium\",\n",
        "    \"لليورانيوم\": \"Uranium\",\n",
        "    \"باليورانيوم\": \"Uranium\",\n",
        "    \"واليورانيوم\": \"Uranium\",\n",
        "    \"يورانيوم\": \"Uranium\",\n",
        "    \"اليورانيوم اليورانيوم\": \"Uranium\",\n",
        "    \"يورانيوم يورانيوم\": \"Uranium\",\n",
        "    \"اليورانيومي\": \"Uranium\",\n",
        "    \"كاليورانيوم\": \"Uranium\",\n",
        "\n",
        "    # Plutonium variations\n",
        "    \"البلوتونيوم\": \"Plutonium\",\n",
        "    \"بلوتونيوم\": \"Plutonium\",\n",
        "    \"البلوتونيوم واليورانيوم\": \"Plutonium and Uranium\",\n",
        "    \"بالبلوتونيوم\": \"Plutonium\",\n",
        "\n",
        "    # Remaining terms\n",
        "    \"جهاز الطرد المركزي الغازي\": \"Gas centrifuge\",\n",
        "    \"فصل النظائر\": \"Isotopic separation\",\n",
        "    \"إعادة المعالجة\": \"Reprocessing\",\n",
        "    \"الثوريوم\": \"Thorium\",\n",
        "    \"التريتيوم\": \"Tritium\",\n",
        "    \"الماء الثقيل\": \"Heavy water\",\n",
        "    \"الكعكة الصفراء\": \"Yellowcake\",\n",
        "    \"مفاعل صغير نمطي\": \"Small modular reactor\",\n",
        "    \"النووي\": \"Nuclear\",\n",
        "    \"بالثوريوم\": \"Thorium\",\n",
        "    \"نووية\": \"Nuclear\"\n",
        "}\n",
        "\n",
        "# Initialize the GoogleTranslator from deep_translator\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "\n",
        "# Function to translate Arabic snippets into English with tqdm progress bar\n",
        "def translate_snippet(snippet):\n",
        "    try:\n",
        "        return translator.translate(snippet)\n",
        "    except Exception as e:\n",
        "        return f\"Translation error: {str(e)}\"\n",
        "\n",
        "# Function to extract entities from English snippets\n",
        "def extract_entities(snippet):\n",
        "    try:\n",
        "        doc = nlp(snippet)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        return entities if entities else \"No entities found\"\n",
        "    except Exception as e:\n",
        "        return f\"NER error: {str(e)}\"\n",
        "\n",
        "# Function to extract key themes based on multiple columns\n",
        "def extract_key_theme(row):\n",
        "    military_keywords = ['weapon', 'missile', 'military', 'defense', 'war', 'attack', 'strike']\n",
        "    energy_keywords = ['energy', 'reactor', 'power', 'electricity', 'fuel', 'generation']\n",
        "    diplomacy_keywords = ['diplomacy', 'negotiation', 'treaty', 'sanctions', 'agreement', 'peace']\n",
        "    safety_keywords = ['safety', 'accident', 'radiation', 'hazard', 'security', 'protocol']\n",
        "    technology_keywords = ['technology', 'innovation', 'infrastructure', 'development', 'research']\n",
        "\n",
        "    combined_text = f\"{row['Mention_English']} {row['English_Snippet']} {row['Key_Entities']}\"\n",
        "\n",
        "    if any(keyword in combined_text.lower() for keyword in military_keywords):\n",
        "        return 'Nuclear Military'\n",
        "    elif any(keyword in combined_text.lower() for keyword in energy_keywords):\n",
        "        return 'Nuclear Energy'\n",
        "    elif any(keyword in combined_text.lower() for keyword in diplomacy_keywords):\n",
        "        return 'Nuclear Diplomacy'\n",
        "    elif any(keyword in combined_text.lower() for keyword in safety_keywords):\n",
        "        return 'Nuclear Safety'\n",
        "    elif any(keyword in combined_text.lower() for keyword in technology_keywords):\n",
        "        return 'Nuclear Technology'\n",
        "    else:\n",
        "        return 'General Nuclear Theme'\n",
        "\n",
        "# Function to extract and prioritize keywords, limited to 10 maximum\n",
        "def extract_keywords(row):\n",
        "    # Combine relevant columns, excluding 'Text search' as it contains Arabic\n",
        "    combined_text = f\"{row['Mention_English']} {row['English_Snippet']} {row['Key_Entities']} {row['Key_Theme']}\"\n",
        "\n",
        "    # Tokenize the combined text and filter out non-alphanumeric tokens and stopwords\n",
        "    try:\n",
        "        tokens = word_tokenize(combined_text.lower())\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "        tokens = word_tokenize(combined_text.lower())\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Count word frequency in the combined text\n",
        "    token_counter = Counter(filtered_tokens)\n",
        "\n",
        "    # Prioritize named entities by boosting their frequency\n",
        "    if isinstance(row['Key_Entities'], list):\n",
        "        for ent in row['Key_Entities']:\n",
        "            if all(c.isalpha() and c.isascii() for c in ent[0]):\n",
        "                token_counter[ent[0].lower()] += 5  # Boost entity importance\n",
        "\n",
        "    # Get the top 10 keywords based on frequency\n",
        "    top_keywords = [word for word, _ in token_counter.most_common(10)]\n",
        "\n",
        "    return top_keywords\n",
        "\n",
        "# Load and process each uploaded file\n",
        "for filename in uploaded_files.keys():\n",
        "    # Determine if the file is CSV or Excel\n",
        "    if filename.endswith('.csv'):\n",
        "        df = pd.read_csv(filename)\n",
        "    elif filename.endswith(('.xlsx', '.xls')):\n",
        "        df = pd.read_excel(filename)\n",
        "    else:\n",
        "        print(f\"Skipping unsupported file type: {filename}\")\n",
        "        continue\n",
        "\n",
        "    # Apply the strict mapping dictionary to replace Arabic phrases with corresponding English keywords\n",
        "    df['Mention_English'] = df['Mention'].map(mapping_dict).fillna(df['Mention'])\n",
        "\n",
        "    # Translate the snippets into English\n",
        "    df['English_Snippet'] = df['Mention in transcript snippet'].progress_apply(translate_snippet)\n",
        "\n",
        "    # Extract entities from English snippets\n",
        "    df['Key_Entities'] = df['English_Snippet'].progress_apply(extract_entities)\n",
        "\n",
        "    # Extract key themes from each row\n",
        "    df['Key_Theme'] = df.progress_apply(extract_key_theme, axis=1)\n",
        "\n",
        "    # Extract and prioritize keywords from each row\n",
        "    df['Key_Words'] = df.progress_apply(extract_keywords, axis=1)\n",
        "\n",
        "    # Add Country and Language columns based on channel source code\n",
        "    df['Country'] = df['Channel source code'].map(lambda x: channel_mapping.get(x, {}).get('Country', 'Unknown'))\n",
        "    df['Language'] = df['Channel source code'].map(lambda x: channel_mapping.get(x, {}).get('Language', 'Unknown'))\n",
        "\n",
        "    # Save the processed DataFrame to a new CSV file\n",
        "    output_filename = f\"processed_{filename}\"\n",
        "    df.to_csv(output_filename, index=False)\n",
        "    print(f\"Processed data saved to {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_6NJmXPlN22",
        "outputId": "bd9189c6-0324-4ed9-bac5-fa285d873a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0KPyJWsBC/A9wZhrqgYrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}